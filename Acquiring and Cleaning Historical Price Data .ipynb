{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc81eaa",
   "metadata": {},
   "source": [
    "# Acquiring and Cleaning Historical Price Data for Bitcoin and other Cryptocurrencies\n",
    "\n",
    "__Niklas Gutheil__<br>\n",
    "\n",
    "__2022-03-01__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868359bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import bitfinex\n",
    "import time\n",
    "import datetime\n",
    "import plotly.express as px\n",
    "# stats\n",
    "from statsmodels.api import tsa # time series analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f9ffc8",
   "metadata": {},
   "source": [
    "## Table of Contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88932b86",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2cedcc",
   "metadata": {},
   "source": [
    "To begin any data science project, we will have to acquire our data. For our purposes we will connect to the Bitfinex API and download Bitcoins Historical Price Data in 1-minute intervals, which we can later aggregate to any other timeframe we wish to have. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a5a6f7",
   "metadata": {},
   "source": [
    "Bitfinex also has a cutsom python library that lets you connect to their public API, which we have installed into our environment using: <br> `pip install bitfinex_tencars`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cf4654",
   "metadata": {},
   "source": [
    "Since this is a public API we won't need to generate an API key, but this comes with limitations such as only being able to make 1 request per second, and the amount of results allowed to be returned for every request is 10,000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc61ea05",
   "metadata": {},
   "source": [
    "The basic information the request needs are: \n",
    "- `pair`: The trading pair ex: BTCUSD\n",
    "- `time_interval`: What time resolution you want the data in ex: 1 minute, 1 hour, 1 day etc.\n",
    "- `t_start`: Start date of requested historical price data\n",
    "- `t_stop`: End date of requested historical price data <br><br>\n",
    "\n",
    "We will download our data in 5-minute intervals as 1-minute intervals aren't neccesary for our inteded purpose of showcasing algorithms, and 1-minute time intervals would create far too many data entries. We will also download all the historical data from January 1st, 2014 to January 1st, 2022. The reason we are choosing 2014 as our starting point is that before this time trading activity looked very different than today. The market conditions before then were more akin to penny stocks, and would thus behave differently than high-volume assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339caa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = 'BTCUSD'\n",
    "\n",
    "time_interval = '5m'    # 15-minute time interval\n",
    "\n",
    "t_start = datetime.datetime(2014, 1, 1, 0, 0)     # January 1st, 2014\n",
    "t_start = time.mktime(t_start.timetuple()) * 1000    # convert starting date to milliseconds since unix epoch\n",
    "\n",
    "t_stop = datetime.datetime(2022, 1, 1, 0, 0)    # January 1st, 2021\n",
    "t_stop = time.mktime(t_stop.timetuple()) * 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edaff68",
   "metadata": {},
   "source": [
    "Now we have to build a function that will collect all of our chunks of data and combine them into a single DataFrame. Since we are using the public REST API, we are limited to 10,000 entries returned per request, and 1 request per second. <br><br>\n",
    "\n",
    "Our function will take in our above parameters and 1 additional parameter `s_inInterval` that will store how many seconds our `time_interval`variable is made up of. This will be important for calcuating how many times we have to call the API to get our full data. <br><br>\n",
    "\n",
    "Our function will return the combined data from all API request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226a88f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(start, stop, symbol, interval, s_inInterval):\n",
    "    limit = 10000    # We want the maximum of 10000 data points\n",
    "\n",
    "    api = bitfinex.bitfinex_v2.api_v2() # Create API instance\n",
    "    \n",
    "    interval_milli = s_inInterval * 1000 # turn our seconds in interval to milliseconds\n",
    "    step = interval_milli * limit # our step size (time interval for each request) will be 10,000 times the single time interval we want our data in\n",
    "    data = []\n",
    "\n",
    "    total_steps = (stop-start)/interval_milli # total number of requests we will have to make\n",
    "    \n",
    "    while total_steps > 0:\n",
    "        if total_steps < limit: # recalculating ending steps\n",
    "            step = total_steps * interval_milli\n",
    "\n",
    "        end = start + step # define endpoint for this request\n",
    "        \n",
    "        data += api.candles(symbol=symbol, interval=interval, limit=limit, start=start, end=end)\n",
    "        print(pd.to_datetime(start, unit='ms'), pd.to_datetime(end, unit='ms'), \"steps left:\", total_steps)\n",
    "        \n",
    "        start = start + step # update new start point for next request\n",
    "        total_steps -= limit # update total_steps left\n",
    "        \n",
    "        time.sleep(1.5) #sleep for 1.5 seconds to make sure we dont time out the API\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc01e8a",
   "metadata": {},
   "source": [
    "Now we can fetch the data using our variables defined earlier, as well as defining our `s_inInterval` which will simply be 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53b3623",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s_inInterval = 300\n",
    "\n",
    "result = fetch_data(t_start, t_stop, pair, time_interval, s_inInterval)\n",
    "names = ['Date', 'Open', 'Close', 'High', 'Low', 'Volume']\n",
    "df = pd.DataFrame(result, columns=names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d3e806",
   "metadata": {},
   "source": [
    "Let's inspect our data to make sure we have our expected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf3f80e",
   "metadata": {},
   "source": [
    "The data looks to be in the form that we were looking for. The columns can be interpreted as follows:\n",
    "- __Date__: Time in Seconds since the Unix Epoch\n",
    "- __Open__: The price of Bitcoin in USD at the beginning of our 5-minute interval\n",
    "- __Close__: The price of Bitcoin in USD at the end of our 5-minute interval\n",
    "- __High__: The highest price of Bitcoin in USD during our 5-minute interval\n",
    "- __Low__: The lowest price of Bitcoin in USD during our 5-minute interval\n",
    "- __Volume__: The amount of Bitcoin bought and sold in our 5-minute interval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47db2164",
   "metadata": {},
   "source": [
    "Now we can take a look at duplicate values and decide if we want to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270eb69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{df.duplicated().sum()} Duplicate Entries')\n",
    "print(f'{np.round(df.duplicated().sum()/df.shape[0]*100, decimals = 2)}% of Entries are Duplicates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4f9ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated(keep = False)].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdfd770",
   "metadata": {},
   "source": [
    "The entries appear to be perfect duplicates as they share the same timestamp, and all other correpsonding values. They also only make up a tiny fraction of our total data, so we can safely drop them. One issue this might cause later is that we have missing values for some timestamps. We will investigate this later on in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c5fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dcb00e",
   "metadata": {},
   "source": [
    "We can also see that our Date column is showing the seconds since the Unix epoch. We will want to convert this column into a DateTime format so we can use this dataset for timeseries specific analysis. Additionally, we will set our index to be our Date column, as each timestamp is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d777be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'], unit='ms')\n",
    "df.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba23f95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f84438",
   "metadata": {},
   "source": [
    "Our columns are looking much better now, but another problem we might have is that we could be missing entries for a specific time interval. Let's explore if and how many we have missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6693b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_time = df.index.min()\n",
    "last_time = df.index.max()\n",
    "print(first_time, last_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0cc26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_range = pd.date_range(start=first_time, end=last_time, freq=\"5min\")\n",
    "differences = full_range.difference(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{100*round(len(differences)/df.shape[0], 6)}% of intervals missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f3492",
   "metadata": {},
   "source": [
    "We can see that only 2.29% of our entries are missing, which is good, as imputing entries won't lower the results by much. First lets check if we have any NULL values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf78a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c0358d",
   "metadata": {},
   "source": [
    "No NULL values is great to see! <br>\n",
    "We should also make sure none of the values are 0, in case Bitfinex imputed null values with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b1f110",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(np.where(df['Volume'] == 0))\n",
    "display(np.where(df['Open'] == 0))\n",
    "display(np.where(df['Close'] == 0))\n",
    "display(np.where(df['High'] == 0))\n",
    "display(np.where(df['Low'] == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f01d9",
   "metadata": {},
   "source": [
    "None of the values are 0 so we are now confident we have real values for our existing entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395dd3b7",
   "metadata": {},
   "source": [
    "Our next step is to create entries for our missing date ranges, and then impute those null values with some kind of value. We could use forwardfilling to simply impute the last known value, but using the mean of the last previous and next known value seems like a more appropriate choice as we are dealing with price information, which has to rise through a given set of values. <br>\n",
    "We will set the method to 'time' for interpolate as this will also account for missing entries of 2 or more in a row. Instead of imputing both consecutive missing time intervals with the same number, it will split this increase over the amount of missing rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3257c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.reindex(full_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842e2de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.interpolate(method='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33bd869",
   "metadata": {},
   "source": [
    "Let's check if we have any missing time intervals or NULL values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6563fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_time = df_clean.index.min()\n",
    "last_time = df_clean.index.max()\n",
    "\n",
    "full_range = pd.date_range(start=first_time, end=last_time, freq=\"5min\")\n",
    "differences = full_range.difference(df_clean.index)\n",
    "\n",
    "df.sort_index(inplace=True) # make sure all the entries are in order of date\n",
    "\n",
    "print(f\"{100*round(len(differences)/df_clean.shape[0], 6)}% of intervals missing\")\n",
    "\n",
    "display(df.isna().sum())\n",
    "\n",
    "print(df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e47002",
   "metadata": {},
   "source": [
    "We now have a fully cleaned dataset with no missing intervals, no null values and 841,537 entries! Let's view Bitcoins historical price data from January 1st, 2014 to January 1st, 2022 in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dfbf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add lines for each column\n",
    "fig = px.line(df, x=df.index, y=df.Open,)\n",
    "\n",
    "# axis labels and title\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"Date\", \n",
    "    xaxis_title=\"Price in USD\",\n",
    "    legend_title=\"\", \n",
    "    title=\"Bitcoin Price Chart for 2014\"\n",
    ")\n",
    "\n",
    "# activate slider\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b60db",
   "metadata": {},
   "source": [
    "Lastly, let's save this data as a csv file for easier later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7de592",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv(f\"{pair}_{time_interval} historical data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
